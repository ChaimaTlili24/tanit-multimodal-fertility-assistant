import gradio as gr
from typing import List, Dict, Any, Optional

from voice.stt import transcribe_audio
from rag.rag import retrieve_context

from vlm import analyze_image, analyze_pdf


# ==============================
#  STUBS / PLACEHOLDERS BACKEND
# ==============================

def simple_stt_transcribe(audio_path: Optional[str]) -> str:
    """
    Wrapper qui utilise le vrai mod√®le faster-whisper d√©fini dans voice/stt.py.
    """
    if audio_path is None:
        return ""
    return transcribe_audio(audio_path)


def simple_image_analysis(image_path: Optional[str], user_text: str) -> str:
    """
    Wrapper autour de vlm.analyze_image().
    """
    return analyze_image(image_path, user_text or "")


def simple_pdf_analysis(pdf_path: Optional[str], user_text: str) -> str:
    """
    Wrapper autour de vlm.analyze_pdf().
    """
    return analyze_pdf(pdf_path, user_text or "")



def simple_rag_answer(user_text: str, extra_context: str) -> str:
    """
    Combine :
    - contexte RAG (en utilisant le texte s'il existe, sinon le contexte multimodal),
    - contexte multimodal (voix / image / PDF),
    - et pr√©pare un message final plus propre.
    """
    base_disclaimer = (
        "‚öïÔ∏è *Je suis un assistant √©ducatif sur la fertilit√© et je ne remplace pas un m√©decin.*\n"
        "Pour toute d√©cision m√©dicale ou traitement, consulte toujours un professionnel de sant√©.\n\n"
    )

    # Si l'utilisateur n'a pas √©crit de texte, on essaie d'utiliser le contexte multimodal
    rag_input = (user_text or "").strip()
    if not rag_input and extra_context:
        rag_input = extra_context

    # Contexte RAG (simple pour l'instant)
    rag_context = retrieve_context(rag_input or "")

    # Texte affich√© dans "Ta question :"
    if user_text and user_text.strip():
        question_block = f"**Ta question :** {user_text}"
    elif extra_context:
        question_block = (
            "**Ta question :** [pas de texte, mais tu as envoy√© un document "
            "que j'analyse en me basant sur son contenu et le contexte m√©dical.]"
        )
    else:
        question_block = "**Ta question :** [aucun contenu re√ßu]"

    response = (
        f"{base_disclaimer}"
        f"**Contexte m√©dical (RAG) :** {rag_context}\n\n"
        f"{question_block}\n\n"
        f"**Contexte multimodal (audio/image/PDF) :** {extra_context or '[aucun]'}\n\n"
        "üí° *Cette r√©ponse est une explication g√©n√©rale bas√©e sur des recommandations "
        "√©ducatives. Elle ne remplace pas une consultation individuelle.*"
    )

    return response




# ==============================
#  LOGIQUE CHATBOT
# ==============================

def chat_pipeline(
    history: List[Dict[str, Any]],
    user_text: str,
    user_audio,
    user_image,
    user_pdf,
):
    """
    Fonction centrale appel√©e par Gradio √† chaque message.

    - history : liste de messages au format [{"role": "...", "content": "..."}, ...]
    - user_text : texte tap√©
    - user_audio : fichier audio (chemin) ou None
    - user_image : image (chemin) ou None
    - user_pdf : PDF (chemin) ou None
    """

    # 1) Transcription audio (STT r√©el)
    transcribed = simple_stt_transcribe(user_audio)

        # 2) Analyse image (via VLM stub pour l'instant)
    image_summary = simple_image_analysis(user_image, user_text)

    # 3) Analyse PDF (stub)
    pdf_summary = simple_pdf_analysis(user_pdf, user_text)


    # 4) Contexte suppl√©mentaire
    extra_parts = []
    if transcribed:
        extra_parts.append(f"Voix: {transcribed}")
    if image_summary:
        extra_parts.append(f"Image: {image_summary}")
    if pdf_summary:
        extra_parts.append(f"PDF: {pdf_summary}")

    extra_context = " | ".join(extra_parts)

    # 5) Texte final de l'utilisateur = texte tap√© + transcription si pr√©sente
    full_user_text = user_text or ""
    if transcribed:
        full_user_text += f"\n\n[+ Transcription voix]: {transcribed}"

    # 6) G√©n√©ration de la r√©ponse (stub RAG+LLM)
    assistant_message = simple_rag_answer(full_user_text, extra_context)

    # 7) Mise √† jour de l'historique au format messages
    if full_user_text.strip():
        history.append({"role": "user", "content": full_user_text})

    history.append({"role": "assistant", "content": assistant_message})

    # On retourne :
    # - l'historique pour le Chatbot
    # - texte vid√©
    # - audio reset
    # - image reset
    # - pdf reset
    return history, "", None, None, None


# ==============================
#  INTERFACE GRADIO
# ==============================

def build_interface():
    with gr.Blocks(title="Tanit Multimodal Fertility Assistant") as demo:
        gr.Markdown(
            """
            # üß¨ Tanit Multimodal Fertility Assistant (Prototype)

            Assistant multimodal pour accompagner les patientes en fertilit√© :
            texte, voix, images m√©dicales et documents.

            > ‚ö†Ô∏è **Attention :** Ce prototype est strictement √©ducatif.  
            > Il ne remplace en aucun cas un avis m√©dical professionnel.
            """
        )

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    label="Conversation",
                    height=500,
                )

            with gr.Column(scale=1):
                gr.Markdown("### Entr√©e utilisateur")

                text_input = gr.Textbox(
                    label="Message texte",
                    placeholder="D√©cris ta situation, tes r√©sultats, tes questions...",
                    lines=4,
                )

                audio_input = gr.Audio(
                    label="Voix (micro)",
                    sources=["microphone"],
                    type="filepath",
                )

                image_input = gr.Image(
                    label="Image m√©dicale (bilan, courbe, √©chographie...)",
                    type="filepath",
                )

                pdf_input = gr.File(
                    label="Rapport / Analyse au format PDF",
                    file_types=[".pdf"],
                )

                submit_btn = gr.Button("Envoyer")
                clear_btn = gr.Button("Effacer la conversation")

        # Bouton "Envoyer"
        submit_btn.click(
            fn=chat_pipeline,
            inputs=[chatbot, text_input, audio_input, image_input, pdf_input],
            outputs=[chatbot, text_input, audio_input, image_input, pdf_input],
        )

        # Bouton "Effacer"
        def clear_all():
            return [], "", None, None, None

        clear_btn.click(
            fn=clear_all,
            inputs=None,
            outputs=[chatbot, text_input, audio_input, image_input, pdf_input],
        )

    return demo


if __name__ == "__main__":
    demo = build_interface()
    demo.launch()
